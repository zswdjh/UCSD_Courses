{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from urllib.request import urlopen\n",
    "import scipy.optimize\n",
    "import random\n",
    "from math import exp\n",
    "from math import log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def parseData(fname):\n",
    "    for l in urlopen(fname):\n",
    "        yield eval(l)\n",
    "\n",
    "print(\"Reading data...\")\n",
    "data = list(parseData(\"http://jmcauley.ucsd.edu/cse190/data/beer/beer_50000.json\"))\n",
    "print(\"done\")\n",
    "\n",
    "def inner(x,y):\n",
    "    return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    feat = [1, datum['review/taste'], datum['review/appearance'], datum['review/aroma'], datum['review/palate'], datum['review/overall']]\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature(datum):\n",
    "    datum_text=datum['review/text'].lower().split()\n",
    "    feat = [1, datum_text.count(\"lactic\"),datum_text.count(\"tart\"),datum_text.count(\"sour\"),datum_text.count(\"citric\"),datum_text.count(\"sweet\"),datum_text.count(\"acid\"),datum_text.count(\"hop\"),datum_text.count(\"fruit\"),datum_text.count(\"salt\"),datum_text.count(\"spicy\")]\n",
    "    return feat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [feature(d) for d in data]\n",
    "y = [d['beer/ABV'] >= 6.5 for d in data]\n",
    "X_train = X[:int(len(X)/3)]\n",
    "X_valid = X[int(len(X)/3):int(2*len(X)/3)]\n",
    "X_test = X[int(2*len(X)/3):]\n",
    "y_train = y[:int(len(y)/3)]\n",
    "y_valid = y[int(len(y)/3):int(2*len(y)/3)]\n",
    "y_test = y[int(2*len(y)/3):]\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8914206247325631\n",
      "1.1386990981142389\n"
     ]
    }
   ],
   "source": [
    "N=len(y_train)\n",
    "coeff_1=N/(2*sum(y_train))\n",
    "coeff_0=N/(2*(N-sum(y_train)))\n",
    "print(coeff_1)\n",
    "print(coeff_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number in training set is  16666\n",
      "num of positive 9348\n",
      "number of negative 7318\n"
     ]
    }
   ],
   "source": [
    "print('the number in training set is ',len(y_train))\n",
    "print('num of positive',sum(y_train))\n",
    "print('number of negative',len(y_train)-sum(y_train))\n",
    "print('coefficient before loglikehood is ',len(y_train)/(2*sum(y_train)))\n",
    "print('coefficient before loglikehood for y =0 is ',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Logistic regression by gradient ascent         #\n",
    "##################################################\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "    loglikelihood = 0\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        if y[i]:\n",
    "            loglikelihood -= log(1 + exp(-logit))*coeff_1\n",
    "        else:\n",
    "            loglikelihood -= (logit+log(1+exp(-logit)))*coeff_0\n",
    "    for k in range(len(theta)):\n",
    "        loglikelihood -= lam * theta[k]*theta[k]\n",
    "  # for debugging\n",
    "  # print(\"ll =\" + str(loglikelihood))\n",
    "    return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "    dl = [0]*len(theta)\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        for k in range(len(theta)):\n",
    "            if y[i]:\n",
    "                dl[k] += X[i][k] * (1 - sigmoid(logit))*coeff_1\n",
    "            if not y[i]:\n",
    "                dl[k] += X[i][k]*(-coeff_0)+X[i][k] * (1 - sigmoid(logit))*coeff_0\n",
    "    for k in range(len(theta)):\n",
    "        dl[k] -= lam*2*theta[k]\n",
    "    return numpy.array([-x for x in dl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Train                                          #\n",
    "##################################################\n",
    "def train(lam):\n",
    "    theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X_train[0]), fprime, pgtol = 10, args = (X_train, y_train, lam))\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Predict                                        #\n",
    "##################################################\n",
    "\n",
    "def performance_valid(theta):\n",
    "    scores_valid = [inner(theta,x) for x in X_valid]\n",
    "    predictions_valid = [s > 0 for s in scores_valid]\n",
    "    correct_valid = [(a==b) for (a,b) in zip(predictions_valid,y_valid)]\n",
    "    acc_valid = sum(correct_valid) * 1.0 / len(correct_valid)\n",
    "    return acc_valid\n",
    "def performance_test(theta):\n",
    "    scores_test = [inner(theta,x) for x in X_test]\n",
    "    predictions_test = [s > 0 for s in scores_test]\n",
    "    correct_test = [(a==b) for (a,b) in zip(predictions_test,y_test)]\n",
    "    acc_test = sum(correct_test) * 1.0 / len(correct_test)\n",
    "    return acc_test\n",
    "def performance_train(theta):\n",
    "    scores_train = [inner(theta,x) for x in X_train]\n",
    "    predictions_train = [s > 0 for s in scores_train]\n",
    "    correct_train = [(a==b) for (a,b) in zip(predictions_train,y_train)]\n",
    "    acc_train = sum(correct_train) * 1.0 / len(correct_train)\n",
    "    return acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier_test(theta):\n",
    "    scores_test = [inner(theta,x) for x in X_test]\n",
    "    predictions_test = [s > 0 for s in scores_test]\n",
    "    true_positive = [1 if a==1 and b==1 else 0 for (a,b) in zip(predictions_test,y_test)]\n",
    "    true_negative = [1 if a==0 and b==0 else 0 for (a,b) in zip(predictions_test,y_test)]\n",
    "    false_positive = [1 if a==1 and b==0 else 0 for (a,b) in zip(predictions_test,y_test)]\n",
    "    false_negative = [1 if a==0 and b==1 else 0 for (a,b) in zip(predictions_test,y_test)]\n",
    "    TP=sum(true_positive)\n",
    "    TN=sum(true_negative)\n",
    "    FP=sum(false_positive)\n",
    "    FN=sum(false_negative)\n",
    "    FPR=FP/(FP+TN)\n",
    "    FNR=FN/(FN+TP)\n",
    "    BER=0.5*(FPR+FNR)\n",
    "    print(\"number of true positive on test is \",TP)\n",
    "    print(\"number of true negative on test is \",TN)\n",
    "    print(\"number of false positive on test is \",FP)\n",
    "    print(\"number of false negative on test is \",FN)\n",
    "    print(\"Balanced Error Rate for test: \",BER)\n",
    "    # also can calculate the length of ttpp = [ 1 for (a,b) in zip(predictions_test,y_test) if a==1 and b==1]\n",
    "def evaluate_classifier_train(theta):\n",
    "    scores_train = [inner(theta,x) for x in X_train]\n",
    "    predictions_train = [s > 0 for s in scores_train]\n",
    "    true_positive = [1 if a==1 and b==1 else 0 for (a,b) in zip(predictions_train,y_train)]\n",
    "    true_negative = [1 if a==0 and b==0 else 0 for (a,b) in zip(predictions_train,y_train)]\n",
    "    false_positive = [1 if a==1 and b==0 else 0 for (a,b) in zip(predictions_train,y_train)]\n",
    "    false_negative = [1 if a==0 and b==1 else 0 for (a,b) in zip(predictions_train,y_train)]\n",
    "    TP=sum(true_positive)\n",
    "    TN=sum(true_negative)\n",
    "    FP=sum(false_positive)\n",
    "    FN=sum(false_negative)\n",
    "    FPR=FP/(FP+TN)\n",
    "    FNR=FN/(FN+TP)\n",
    "    BER=0.5*(FPR+FNR)\n",
    "    #print(\"number of true positive on test is \",TP)\n",
    "    #print(\"number of true negative on test is \",TN)\n",
    "    #print(\"number of false positive on test is \",FP)\n",
    "    #print(\"number of false negative on test is \",FN)\n",
    "    print(\"Balanced Error Rate for train: \",BER)\n",
    "def evaluate_classifier_valid(theta):\n",
    "    scores_valid = [inner(theta,x) for x in X_valid]\n",
    "    predictions_valid = [s > 0 for s in scores_valid]\n",
    "    true_positive = [1 if a==1 and b==1 else 0 for (a,b) in zip(predictions_valid,y_valid)]\n",
    "    true_negative = [1 if a==0 and b==0 else 0 for (a,b) in zip(predictions_valid,y_valid)]\n",
    "    false_positive = [1 if a==1 and b==0 else 0 for (a,b) in zip(predictions_valid,y_valid)]\n",
    "    false_negative = [1 if a==0 and b==1 else 0 for (a,b) in zip(predictions_valid,y_valid)]\n",
    "    TP=sum(true_positive)\n",
    "    TN=sum(true_negative)\n",
    "    FP=sum(false_positive)\n",
    "    FN=sum(false_negative)\n",
    "    FPR=FP/(FP+TN)\n",
    "    FNR=FN/(FN+TP)\n",
    "    BER=0.5*(FPR+FNR)\n",
    "    #print(\"number of true positive on test is \",TP)\n",
    "    #print(\"number of true negative on test is \",TN)\n",
    "    #print(\"number of false positive on test is \",FP)\n",
    "    #print(\"number of false negative on test is \",FN)\n",
    "    print(\"Balanced Error Rate for valid: \",BER)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 1.0:\taccuracy for validation set is\t0.413031739365\n",
      "lambda = 1.0:\taccuracy for test set is\t0.59998800024\n",
      "number of true positive on test is  2378\n",
      "number of true negative on test is  7622\n",
      "number of false positive on test is  3133\n",
      "number of false negative on test is  3534\n",
      "Balanced Error Rate for test:  0.4445368110876459\n",
      "Balanced Error Rate for train:  0.4432375407886931\n",
      "Balanced Error Rate for valid:  0.4199156034378244\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# Validation pipeline                            #\n",
    "##################################################\n",
    "lam = 1.0\n",
    "theta = train(lam)\n",
    "acc_valid = performance_valid(theta)\n",
    "acc_test = performance_test(theta)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy for validation set is\\t\" + str(acc_valid))\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy for test set is\\t\" + str(acc_test))\n",
    "\n",
    "evaluate_classifier_test(theta)\n",
    "evaluate_classifier_train(theta)\n",
    "evaluate_classifier_valid(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 0:\taccuracy for train set is\t0.53726149046\n",
      "lambda = 0:\taccuracy for validation set is\t0.412971740565\n",
      "lambda = 0:\taccuracy for test set is\t0.59998800024\n",
      "lambda = 0.01:\taccuracy for train set is\t0.53726149046\n",
      "lambda = 0.01:\taccuracy for validation set is\t0.412971740565\n",
      "lambda = 0.01:\taccuracy for test set is\t0.59998800024\n",
      "lambda = 0.1:\taccuracy for train set is\t0.53726149046\n",
      "lambda = 0.1:\taccuracy for validation set is\t0.412971740565\n",
      "lambda = 0.1:\taccuracy for test set is\t0.59998800024\n"
     ]
    }
   ],
   "source": [
    "#(5)\n",
    "lam = [0,0.01,0.1]\n",
    "for i in lam:\n",
    "    theta = train(i)\n",
    "    acc_train = performance_train(theta)\n",
    "    acc_valid = performance_valid(theta)\n",
    "    acc_test = performance_test(theta)\n",
    "    print(\"lambda = \" + str(i) + \":\\taccuracy for train set is\\t\" + str(acc_train))\n",
    "    print(\"lambda = \" + str(i) + \":\\taccuracy for validation set is\\t\" + str(acc_valid))\n",
    "    print(\"lambda = \" + str(i) + \":\\taccuracy for test set is\\t\" + str(acc_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of true positive on test is  3920\n",
      "number of true negative on test is  4940\n",
      "number of false positive on test is  5815\n",
      "number of false positive on test is  1992\n",
      "Balanced Error Rate:  0.4388102836645196\n",
      "3920\n"
     ]
    }
   ],
   "source": [
    "    scores_test = [inner(theta,x) for x in X_test]\n",
    "    predictions_test = [s > 0 for s in scores_test]\n",
    "    true_positive = [1 if a==1 and b==1 else 0 for (a,b) in zip(predictions_test,y_test)]\n",
    "    true_negative = [1 if a==0 and b==0 else 0 for (a,b) in zip(predictions_test,y_test)]\n",
    "    false_positive = [1 if a==1 and b==0 else 0 for (a,b) in zip(predictions_test,y_test)]\n",
    "    false_negative = [1 if a==0 and b==1 else 0 for (a,b) in zip(predictions_test,y_test)]\n",
    "    TP=sum(true_positive)\n",
    "    TN=sum(true_negative)\n",
    "    FP=sum(false_positive)\n",
    "    FN=sum(false_negative)\n",
    "    FPR=FP/(FP+TN)\n",
    "    FNR=FN/(FN+TP)\n",
    "    BER=0.5*(FPR+FNR)\n",
    "    print(\"number of true positive on test is \",TP)\n",
    "    print(\"number of true negative on test is \",TN)\n",
    "    print(\"number of false positive on test is \",FP)\n",
    "    print(\"number of false positive on test is \",FN)\n",
    "    print(\"Balanced Error Rate: \",BER)\n",
    "    ttpp = [ 1 for (a,b) in zip(predictions_test,y_test) if a==1 and b==1]\n",
    "    print(len(ttpp))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
