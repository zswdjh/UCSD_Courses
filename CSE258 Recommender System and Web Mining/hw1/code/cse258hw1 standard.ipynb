{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n",
      "Variance = 0.513569937499659\n",
      "defaultdict(<class 'int'>, {'Hefeweizen': 618, 'English Strong Ale': 164, 'Foreign / Export Stout': 55, 'German Pilsener': 586, 'American Double / Imperial IPA': 3886, 'Herbed / Spiced Beer': 73, 'Oatmeal Stout': 102, 'American Pale Lager': 123, 'Rauchbier': 1938, 'American Pale Ale (APA)': 2288, 'American Porter': 2230, 'Belgian Strong Dark Ale': 146, 'Russian Imperial Stout': 2695, 'American Amber / Red Ale': 665, 'American Strong Ale': 166, 'MÃ¤rzen / Oktoberfest': 557, 'American Adjunct Lager': 242, 'American Blonde Ale': 357, 'American IPA': 4113, 'Fruit / Vegetable Beer': 1355, 'English Bitter': 267, 'English Porter': 367, 'Irish Dry Stout': 101, 'American Barleywine': 825, 'American Double / Imperial Stout': 5964, 'Doppelbock': 873, 'American Stout': 591, 'Maibock / Helles Bock': 225, 'Dortmunder / Export Lager': 31, 'Euro Strong Lager': 329, 'Low Alcohol Beer': 7, 'Light Lager': 503, 'Euro Pale Lager': 701, 'Bock': 148, 'English India Pale Ale (IPA)': 175, 'Altbier': 165, 'KÃ¶lsch': 94, 'Pumpkin Ale': 560, 'Rye Beer': 1798, 'American Pale Wheat Ale': 154, 'Milk / Sweet Stout': 69, 'Schwarzbier': 53, 'Munich Dunkel Lager': 141, 'Vienna Lager': 33, 'American Amber / Red Lager': 42, 'Scottish Ale': 78, 'Witbier': 162, 'Saison / Farmhouse Ale': 141, 'American Black Ale': 138, 'English Brown Ale': 495, 'English Barleywine': 133, 'Extra Special / Strong Bitter (ESB)': 667, 'California Common / Steam Beer': 11, 'Euro Dark Lager': 144, 'Scotch Ale / Wee Heavy': 2776, 'English Pale Ale': 1324, 'Belgian Strong Pale Ale': 632, 'Belgian Pale Ale': 144, 'Tripel': 257, 'Flanders Oud Bruin': 13, 'American Brown Ale': 314, 'Smoked Beer': 61, 'Dunkelweizen': 61, 'Dubbel': 165, 'Keller Bier / Zwickel Bier': 23, 'Winter Warmer': 259, 'BiÃ¨re de Garde': 7, 'Belgian Dark Ale': 175, 'Irish Red Ale': 83, 'Chile Beer': 11, 'English Stout': 136, 'Czech Pilsener': 1501, 'Belgian IPA': 128, 'Black & Tan': 122, 'Cream Ale': 69, 'English Dark Mild Ale': 21, 'American Wild Ale': 98, 'Weizenbock': 13, 'American Double / Imperial Pilsner': 14, 'Scottish Gruit / Ancient Herbed Ale': 65, 'Wheatwine': 455, 'American Dark Wheat Ale': 14, 'American Malt Liquor': 90, 'Munich Helles Lager': 650, 'Kristalweizen': 7, 'English Pale Mild Ale': 21, 'Baltic Porter': 514, 'Old Ale': 1052, 'Quadrupel (Quad)': 119, 'Braggot': 26, 'Lambic - Fruit': 6, 'Lambic - Unblended': 10, 'Eisbock': 8, 'Flanders Red Ale': 2, 'Berliner Weissbier': 10})\n",
      "theta = [ 3.91520474  0.08564622]\n",
      "Train MSE = 0.558107286559, test MSE = 0.468410050967\n",
      "Train MSE = 0.55597473573, test MSE = 0.433666506397\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from urllib.request import urlopen\n",
    "import scipy.optimize\n",
    "import random\n",
    "from sklearn import svm\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "def parseData(fname):\n",
    "      for l in urlopen(fname):\n",
    "        yield eval(l)\n",
    "\n",
    "print(\"Reading data...\")\n",
    "data = list(parseData(\"http://jmcauley.ucsd.edu/cse190/data/beer/beer_50000.json\"))\n",
    "print(\"done\")\n",
    "\n",
    "##################################################\n",
    "# Helper functions                               #\n",
    "##################################################\n",
    "\n",
    "def inner(x, y):\n",
    "    sum = 0\n",
    "    for a,b in zip(x,y):\n",
    "        sum += a*b\n",
    "    return sum\n",
    "\n",
    "def squaredDiff(x, y):\n",
    "    sum = 0\n",
    "    for a,b in zip(x,y):\n",
    "        sum += (a-b)*(a-b)\n",
    "    return sum\n",
    "\n",
    "##################################################\n",
    "# Simple statistics                              #\n",
    "##################################################\n",
    "\n",
    "# Find the variance of the 'review/taste' value\n",
    "\n",
    "y = [d['review/taste'] for d in data]\n",
    "\n",
    "mean = sum(y) * 1.0 / len(y)\n",
    "\n",
    "mse_test = squaredDiff([mean for d in data], y) / len(y)\n",
    "\n",
    "print(\"Variance = \" + str(mse_test))\n",
    "\n",
    "styleCounts = defaultdict(int)\n",
    "\n",
    "for d in data:\n",
    "    styleCounts[d['beer/style']] += 1\n",
    "\n",
    "print(styleCounts)\n",
    "\n",
    "##################################################\n",
    "# Regression on style                            #\n",
    "##################################################\n",
    "\n",
    "def feature(datum):\n",
    "    isAIPA = 0\n",
    "    if datum['beer/style'] == 'American IPA':\n",
    "        isAIPA = 1\n",
    "    feat = [1, isAIPA]\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['review/taste'] for d in data]\n",
    "\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(X, y)\n",
    "\n",
    "print(\"theta = \" + str(theta))\n",
    "\n",
    "##################################################\n",
    "# Train/test splits                              #\n",
    "##################################################\n",
    "\n",
    "X_train = X[:int(len(X)/2)]\n",
    "X_test = X[int(len(X)/2):]\n",
    "\n",
    "y_train = y[:int(len(y)/2)]\n",
    "y_test = y[int(len(y)/2):]\n",
    "\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(X_train, y_train)\n",
    "\n",
    "predictions_train = [inner(x,theta) for x in X_train]\n",
    "mse_train = squaredDiff(predictions_train, y_train) / len(y_train)\n",
    "\n",
    "predictions_test = [inner(x,theta) for x in X_test]\n",
    "mse_test = squaredDiff(predictions_test, y_test) / len(y_test)\n",
    "\n",
    "print(\"Train MSE = \" + str(mse_train) + \", test MSE = \" + str(mse_test))\n",
    "\n",
    "##################################################\n",
    "# Experiments for every style (CSE258)           #\n",
    "##################################################\n",
    "\n",
    "commonStyles = [s for s in styleCounts if styleCounts[s] >= 50]\n",
    "styleInds = dict(zip(list(commonStyles), range(len(commonStyles))))\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [1] + [0]*len(styleInds)\n",
    "    if datum['beer/style'] in styleInds:\n",
    "        feat[styleInds[datum['beer/style']]] = 1\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "X_train = X[:int(len(X)/2)]\n",
    "X_test = X[int(len(X)/2):]\n",
    "\n",
    "theta,residuals,rank,s = numpy.linalg.lstsq(X_train, y_train)\n",
    "\n",
    "predictions_train = [inner(x,theta) for x in X_train]\n",
    "mse_train = squaredDiff(predictions_test, y_train) / len(y_train)\n",
    "\n",
    "predictions_test = [inner(x,theta) for x in X_test]\n",
    "mse_test = squaredDiff(predictions_test, y_test) / len(y_test)\n",
    "\n",
    "print(\"Train MSE = \" + str(mse_train) + \", test MSE = \" + str(mse_test))\n",
    "\n",
    "##################################################\n",
    "# Classification                                 #\n",
    "##################################################\n",
    "\n",
    "y = [d['beer/style'] == 'American IPA' for d in data]\n",
    "y_train = y[:int(len(y)/2)]\n",
    "y_test = y[int(len(y)/2):]\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [1, datum['beer/ABV'], datum['review/taste']]\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "X_train = X[:int(len(X)/2)]\n",
    "X_test = X[int(len(X)/2):]\n",
    "\n",
    "for C in [0.1,10,1000,100000]:\n",
    "    clf = svm.SVC(C=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    test_predictions = clf.predict(X_test)\n",
    "    test_labels = [a > 0 for a in test_predictions]\n",
    "    test_correct = [(a == b) for a,b in zip(test_labels, y_test)]\n",
    "    print(\"C=\" + str(C) + \": Accuracy = \" + str(sum(test_correct) * 1.0 / len(test_correct)))\n",
    "\n",
    "# Better feature?\n",
    "def feature(datum):\n",
    "    feat = [1, datum['beer/ABV'], datum['review/taste'], 'IPA' in d['review/text'], 'hops' in d['review/text']]\n",
    "    return feat  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Logistic regression                            #\n",
    "##################################################\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + math.exp(-x))\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "    loglikelihood = 0\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        loglikelihood -= math.log(1 + math.exp(-logit))\n",
    "        if not y[i]:\n",
    "            loglikelihood -= logit\n",
    "    for k in range(len(theta)):\n",
    "        loglikelihood -= lam * theta[k]*theta[k]\n",
    "      # for debugging\n",
    "        print(\"ll = \" + str(loglikelihood))\n",
    "    return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "    dl = [0]*len(theta)\n",
    "    for i in range(len(X)):\n",
    "        logit = inner(X[i], theta)\n",
    "        for k in range(len(theta)):\n",
    "            dl[k] += X[i][k] * (1 - sigmoid(logit))\n",
    "            if not y[i]:\n",
    "                dl[k] -= X[i][k]\n",
    "        for k in range(len(theta)):\n",
    "            dl[k] -= lam*2*theta[k]\n",
    "    return numpy.array([-x for x in dl])\n",
    "\n",
    "lam = 1.0\n",
    "theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, pgtol = 10, args = (X_train, y_train, lam))\n",
    "\n",
    "test_predictions = [inner(x,theta) for x in X_test]\n",
    "test_labels = [a > 0 for a in test_predictions]\n",
    "test_correct = [(a == b) for a,b in zip(test_labels, y_test)]\n",
    "\n",
    "print(\"Accuracy = \" + str(sum(test_correct) * 1.0 / len(test_correct)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Validation pipeline                            #\n",
    "##################################################\n",
    "\n",
    "X_train = X[:int(len(X)/2)]\n",
    "y_train = y[:int(len(y)/2)]\n",
    "X_valid = X[int(len(X)/2):int(3*len(X)/4)]\n",
    "y_valid = y[int(len(y)/2):int(3*len(y)/4)]\n",
    "X_test = X[int(3*len(X)/4):]\n",
    "y_test = y[int(3*len(X)/4):]\n",
    "\n",
    "for c in [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]:\n",
    "    clf = svm.SVC(C = c)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_predictions = clf.predict(X_train)\n",
    "    train_labels = [a > 0 for a in train_predictions]\n",
    "    train_correct = [(a == b) for a,b in zip(train_labels, y_train)]\n",
    "    acc_train = sum(train_correct) * 1.0 / len(train_correct)\n",
    "    valid_predictions = clf.predict(X_valid)\n",
    "    valid_labels = [a > 0 for a in valid_predictions]\n",
    "    valid_correct = [(a == b) for a,b in zip(valid_labels, y_valid)]\n",
    "    acc_valid = sum(valid_correct) * 1.0 / len(valid_correct)\n",
    "    test_predictions = clf.predict(X_test)\n",
    "    test_labels = [a > 0 for a in test_predictions]\n",
    "    test_correct = [(a == b) for a,b in zip(test_labels, y_test)]\n",
    "    acc_test = sum(test_correct) * 1.0 / len(test_correct)\n",
    "    print(\"c = \" + str(c) + \";\\ttrain=\" + str(acc_train) + \"; validate=\" + str(acc_valid) + \"; test=\" + str(acc_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
